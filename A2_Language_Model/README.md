# Assignment A2 : Language Model
Student ID: st125214

# About this task
This task uses a text paragraphs to train a LSTM Language Model
A few tweaking experiments will be needed to perform to create the best Train and Valid Perplexity values.
Due to the compute resource limitations to thain and experiment model, our task only focuses on the 
procedures of LSTM model training. The results are just to demonstrate, not to be used in production.
For practical use, we need to use larger text dataset, and increase the training parameters to get best results.

============================
## Task 1 : Data Acquisition
============================
## Source Material
1. I use 13 James Bond stories by Ian Fleming as a corpus data.
   Original ebook file : I use the 13 original ebook files from the below data source.
3. Data source: https://archive.org/details/james-bond-ian-fleming_202303/01%20Casino%20Royale%20-%20Ian%20Fleming/
   Remarks : Not all the archived contents from this data source are in public domain. Users should check the material
             whether those are copyrighted or under public domain before using those materials to avoid copyright issues.
 
## Preparing the source material
1. Since the source material is in EPUB ebook format with formatting and images, processing is needed to convert to text-only.
   I use calibre ebook manager application to convert the EPUB ebook into text-only format.
2. A python preprocessing code was used to read each txt file, and combine those into a single txt file.

==========================
## Task 2 : Model Training
==========================
## Data preprocessing
1. Since the orignnal dataset is in a single text file, which we assume as 'train' set, we will need to split the train set
   into different 'train', 'validation', and 'test' sets.
2. Code using train_test_split was used to splitting the dataset. The test and validation sets were assigned .15% of the main set.

## Model Architectures

The model is an **LSTM-based Language Model** implemented in PyTorch. The model has following key components:

1. **Embedding Layer**:

2. **LSTM Layer**:

3. **Dropout Layer**:

4. **Fully Connected Layer**:

5. **Initialization**:

## Training Process

### Data Preparation
1. **Tokenization and Numericalization**:

2. **Batching**:

### Training Loop
2. **Forward Pass**:

3. **Loss Calculation**:

4. **Backpropagation**:

5. **Evaluation**:

6. **Checkpointing**:

## Hyperparameters
- **`vocab_size`**: Size of the vocabulary.(12260)
- **`emb_dim`**: Dimension of the embedding layer (1024).
- **`hid_dim`**: Dimension of the LSTM hidden state (1024).
- **`num_layers`**: Number of LSTM layers (2).
- **`dropout_rate`**: Dropout probability (0.65).
- **`batch_size`**: Number of sequences per batch (128).
- **`seq_len`**: Length of each sequence (50).
- **`clip`**: Gradient clipping threshold (0.25).
- **`lr`**: Learning rate (1e-3).
- **`n_epochs`**: Number of training epochs (25).

## Metrics
- **Perplexity**:

## Training Output
During training, the following metrics are printed for each epoch:
- **Train Perplexity**: Perplexity on the training set.
- **Valid Perplexity**: Perplexity on the validation set.

Total training time is calculated and later displayed (for the sake of experimentation).


=============================================
## Task 3 : Web Application - Text Generation
=============================================
## Application Development
Application was developed with flask and python, with css template as frontend.

## How to run the web app
from the command prompt, run : "python app/app.py"
Access the app from http://127.0.0.1:5000

## Requirement of model's pre-trained state dictionary
The model's pre-trained state dictionary file is 158MB, and cannot be placed in github.
For this, I placed a code segment to download the file from google drive using 'gdown' library, 
where the state dictionary file is already uploaded and shared.
The pre-trained state dictionary file can be downloadable from this link : 
https://drive.google.com/file/d/1vSoQKY5adKdb9FVN9TMDTDso8_qABI1H/

## How to use website
1. Enter the start of paraphrase in the dialog box and the program will generate an continue the text prompt.
2. Optionally, you can specify how long the text continuation process will perform [default value will be 30 words].
3. Five text strings will be generated by model, sorted in ascending order. 
   The highest one will be the sentence that makes the most sense [according to the trained LSTM model].

## Result Screenshots
Sample results screenshots were also placed under the folder 'screens'.

