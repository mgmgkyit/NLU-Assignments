{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Language Models\n",
    "\n",
    "Experimentation of LSTM Language models with a sample text corpus, and try to generate the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment A2 : st125214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch and torchtext compatiblilty\n",
    "\n",
    "Torch has compatibility requirements with torchtext.\n",
    "\n",
    "I prepare my python environment first for the compatibality.\n",
    "\n",
    "Torch version       = 2.2.0+cu118\n",
    "\n",
    "Torchtext version   = 0.16.2+cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "torch.backends.cudnn.deterministic = False # Choose this because speed is a priority and exact reproducibility isn't critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchText version: 0.16.2+cpu\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "print(\"TorchText version:\", torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# assign the device as cuda if available, else cpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 69\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data - James Bond Books\n",
    "\n",
    "We will be using 13 James Books books written by ian fleming, which contains a large corpus of text, perfect for language modeling task.  This time, we will use the `datasets` library from HuggingFace to load.\n",
    "\n",
    "Total books - 13 novels\n",
    "\n",
    "Size of finished text file ~ 5 Mb\n",
    "\n",
    "Number of rows (vectors) - 36,213"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16695ce08e29482eb086fbafe4afe52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 35890\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Marked out the file combination code segment here\n",
    "'''\n",
    "# Combine all text files in the 'james bond' folder into a single text file\n",
    "folder_path = 'james bond'\n",
    "combined_text = ''\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "            combined_text += file.read() + '\\n'\n",
    "\n",
    "# Save the combined text to a single file\n",
    "with open('combined_james_bond.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(combined_text)\n",
    "'''\n",
    "\n",
    "# load the saved combined text file\n",
    "dataset = datasets.load_dataset('text', data_files={'train': 'combined_james_bond.txt'})\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35890, 1)\n"
     ]
    }
   ],
   "source": [
    "# determine how many text vectors (rows) are in the dataset\n",
    "print(dataset['train'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data set into Train, Validate and Test\n",
    "\n",
    "Since the original text file is imported only as 'train', we will split it into train, validate and test.\n",
    "\n",
    "We will first split main set into train and test, test set occupying 15% of main set.\n",
    "\n",
    "Later, train set is split again into train and validate set, validate set occupying 15% of train set.\n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "Simply tokenize the given text to tokens.\n",
    "\n",
    "Uses torchtext to get a simple English tokenizer, which tokenizes text based on basic rules (e.g., splitting by spaces and handling punctuation).\n",
    "\n",
    "It returns a dictionary containing the tokenized output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 25930\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4576\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 5384\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: First split (85% train, 15% test)\n",
    "split1 = dataset['train'].train_test_split(test_size=0.15, seed=SEED)\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': split1['train'],\n",
    "    'test': split1['test']  # 15% test set stored\n",
    "})\n",
    "\n",
    "# Step 2: Further split train into train-validation (85% train, 15% validation)\n",
    "split2 = dataset['train'].train_test_split(test_size=0.15, seed=SEED)\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': split2['train'],  \n",
    "    'validation': split2['test'],\n",
    "    'test': dataset['test']  \n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7504cd29be304665ab5bdaae8ae3272f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25930 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45282f3221394dae939bf77f22327027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4576 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1391ef0763ef4ca2a64d1d84e498b4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the text data using the basic_english tokenizer\n",
    "\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], ['bond', 'took', 'the', 'sharp', 'corner', 'and', 'accelerated', 'up', 'to', 'fifty', '.', 'the', 'viaduct', 'carrying', 'the', 'paris', 'autoroute', 'loomed', 'up', 'ahead', '.', 'the', 'dark', 'mouth', 'of', 'the', 'tunnel', 'beneath', 'it', 'opened', 'and', 'swallowed', 'him', '.', 'the', 'noise', 'of', 'his', 'exhaust', 'was', 'gigantic', ',', 'and', 'for', 'an', 'instant', 'there', 'was', 'a', 'tunnel', 'smell', 'of', 'cold', 'and', 'damp', '.', 'then', 'he', 'was', 'out', 'in', 'the', 'sunshine', 'again', 'and', 'immediately', 'across', 'the', 'carrefour', 'royal', '.', 'ahead', 'the', 'oily', 'tarmac', 'glittered', 'dead', 'straight', 'for', 'two', 'miles', 'through', 'the', 'enchanted', 'forest', 'and', 'there', 'was', 'a', 'sweet', 'smell', 'of', 'leaves', 'and', 'dew', '.', 'bond', 'cut', 'his', 'speed', 'to', 'forty', '.', 'the', 'driving-mirror', 'by', 'his', 'left', 'hand', 'shivered', 'slightly', 'with', 'his', 'speed', '.', 'it', 'showed', 'nothing', 'but', 'an', 'empty', 'unfurling', 'vista', 'of', 'road', 'between', 'lines', 'of', 'trees', 'that', 'curled', 'away', 'behind', 'him', 'like', 'a', 'green', 'wake', '.', 'no', 'sign', 'of', 'the', 'killer', '.', 'had', 'he', 'taken', 'fright', '?', 'had', 'there', 'been', 'some', 'hitch', '?', 'but', 'then', 'there', 'was', 'a', 'tiny', 'black', 'speck', 'in', 'the', 'centre', 'of', 'the', 'convex', 'glass', '-', 'a', 'midge', 'that', 'became', 'a', 'fly', 'and', 'then', 'a', 'bee', 'and', 'then', 'a', 'beetle', '.', 'now', 'it', 'was', 'a', 'crash', 'helmet', 'bent', 'low', 'over', 'handlebars', 'between', 'two', 'big', 'black', 'paws', '.', 'god', ',', 'he', 'was', 'coming', 'fast', '!', 'bond', \"'\", 's', 'eyes', 'flickered', 'from', 'the', 'mirror', 'to', 'the', 'road', 'ahead', 'and', 'back', 'to', 'the', 'mirror', '.', 'when', 'the', 'killer', \"'\", 's', 'right', 'hand', 'went', 'for', 'his', 'gun', '.', '.', '.', '!']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['train'][22:25]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing\n",
    "\n",
    "We will tell torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big.  Also we shall make sure to add `unk` and `eos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], min_freq=3)\n",
    "vocab.insert_token('<unk>', 0)\n",
    "vocab.insert_token('<eos>', 1)\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11932\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<eos>', '.', 'the', ',', 'and', 'of', \"'\", 'a', 'to']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the batch loader\n",
    "\n",
    "### Prepare data\n",
    "\n",
    "Data was prepared using batch size of 128, and tokenized sets for train, validate and test sets were created.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert text data to numerical data from the vocabulary\n",
    "\n",
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in dataset:\n",
    "        if example['tokens']:\n",
    "            tokens = example['tokens'].append('<eos>')\n",
    "            tokens = [vocab[token] for token in example['tokens']]\n",
    "            data.extend(tokens)\n",
    "    data = torch.LongTensor(data)\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "    data = data.view(batch_size, num_batches) #view vs. reshape (whether data is contiguous)\n",
    "    return data #[batch size, seq len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'validation', 'test'])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'],  vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6117])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Architecture**\n",
    "\n",
    "The model is an **LSTM-based Language Model** implemented in PyTorch. The model has following key components:\n",
    "\n",
    "1. **Embedding Layer**:\n",
    "   - Converts token indices into dense vectors of size `emb_dim`.\n",
    "   - Input: `[batch_size, seq_len]`\n",
    "   - Output: `[batch_size, seq_len, emb_dim]`\n",
    "\n",
    "2. **LSTM Layer**:\n",
    "   - Processes the sequence of embeddings to capture contextual information.\n",
    "   - Input: `[batch_size, seq_len, emb_dim]`\n",
    "   - Output:\n",
    "     - `output`: `[batch_size, seq_len, hid_dim]` (hidden states for each time step)\n",
    "     - `hidden`: `[num_layers, batch_size, hid_dim]` (final hidden state and cell state)\n",
    "\n",
    "3. **Dropout Layer**:\n",
    "   - Applied after the embedding and LSTM layers to prevent overfitting.\n",
    "\n",
    "4. **Fully Connected Layer**:\n",
    "   - Maps the LSTM output to the vocabulary size (`vocab_size`).\n",
    "   - Input: `[batch_size, seq_len, hid_dim]`\n",
    "   - Output: `[batch_size, seq_len, vocab_size]`\n",
    "\n",
    "5. **Initialization**:\n",
    "   - Weights are initialized uniformly to small values for stable training.\n",
    "\n",
    "\n",
    "**Training Process**\n",
    "\n",
    "### Data Preparation\n",
    "1. **Tokenization and Numericalization**:\n",
    "   - Text data is tokenized and converted into integer indices using a vocabulary.\n",
    "   - Special tokens like `<eos>` (end of sequence) are added.\n",
    "\n",
    "2. **Batching**:\n",
    "   - Numericalized data is split into batches of size `batch_size`.\n",
    "   - Each batch is divided into sequences of length `seq_len`.\n",
    "\n",
    "### Training Loop\n",
    "1. **Initialization**:\n",
    "   - The LSTM hidden state is initialized to zeros at the start of each epoch.\n",
    "\n",
    "2. **Forward Pass**:\n",
    "   - The model predicts the next token for each position in the sequence.\n",
    "   - The output is reshaped for the loss function.\n",
    "\n",
    "3. **Loss Calculation**:\n",
    "   - **CrossEntropyLoss** computes the difference between predicted and actual tokens.\n",
    "   - Loss is averaged over the sequence length.\n",
    "\n",
    "4. **Backpropagation**:\n",
    "   - Gradients are computed and clipped to prevent exploding gradients.\n",
    "   - The optimizer updates the model parameters.\n",
    "\n",
    "5. **Evaluation**:\n",
    "   - The model is evaluated on the validation set after each epoch.\n",
    "   - The learning rate is adjusted using a scheduler if the validation loss plateaus.\n",
    "\n",
    "6. **Checkpointing**:\n",
    "   - The model with the best validation loss is saved.\n",
    "\n",
    "\n",
    "## Hyperparameters\n",
    "- **`vocab_size`**: Size of the vocabulary.(12260)\n",
    "- **`emb_dim`**: Dimension of the embedding layer (1024).\n",
    "- **`hid_dim`**: Dimension of the LSTM hidden state (1024).\n",
    "- **`num_layers`**: Number of LSTM layers (2).\n",
    "- **`dropout_rate`**: Dropout probability (0.65).\n",
    "- **`batch_size`**: Number of sequences per batch (128).\n",
    "- **`seq_len`**: Length of each sequence (50).\n",
    "- **`clip`**: Gradient clipping threshold (0.25).\n",
    "- **`lr`**: Learning rate (1e-3).\n",
    "- **`n_epochs`**: Number of training epochs (25).\n",
    "\n",
    "---\n",
    "\n",
    "## Metrics\n",
    "- **Perplexity**:\n",
    "  - Used to evaluate the model's performance.\n",
    "  - Defined as `exp(loss)`.\n",
    "  - Lower perplexity indicates better performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Output\n",
    "During training, the following metrics are printed for each epoch:\n",
    "- **Train Perplexity**: Perplexity on the training set.\n",
    "- **Valid Perplexity**: Perplexity on the validation set.\n",
    "\n",
    "Total training time is calculated and later displayed (for the sake of experimentation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim    = hid_dim\n",
    "        self.emb_dim    = emb_dim\n",
    "        \n",
    "        self.embedding  = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout    = nn.Dropout(dropout_rate)\n",
    "        self.fc         = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_other)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #We\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim,   \n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #Wh\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "        \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach() #not to be used for gradient computation\n",
    "        cell   = cell.detach()\n",
    "        return hidden, cell\n",
    "        \n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch_size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src)) #harry potter is\n",
    "        #embedding: [batch-size, seq len, emb dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        #ouput: [batch size, seq len, hid dim]\n",
    "        #hidden: [num_layers * direction, seq len, hid_dim]\n",
    "        output = self.dropout(output)\n",
    "        prediction =self.fc(output)\n",
    "        #prediction: [batch_size, seq_len, vocab_size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training \n",
    "\n",
    "Follows very basic procedure.  One note is that some of the sequences that will be fed to the model may involve parts from different sequences in the original dataset or be a subset of one (depending on the decoding length). For this reason we will reset the hidden state every epoch, this is like assuming that the next batch of sequences is probably always a follow up on the previous in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024                # 400 in the paper\n",
    "hid_dim = 1024                # 1150 in the paper\n",
    "num_layers = 2                # 3 in the paper\n",
    "dropout_rate = 0.65              \n",
    "lr = 1e-3                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model hyperparameters for later use\n",
    "import pickle\n",
    "\n",
    "Data = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'emb_dim': emb_dim,\n",
    "    'hid_dim': hid_dim,\n",
    "    'num_layers': num_layers,\n",
    "    'dropout_rate': dropout_rate,\n",
    "    'tokenizer': tokenizer,\n",
    "    'vocab': vocab\n",
    "}\n",
    "\n",
    "pickle.dump(Data,open('Data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 41,242,268 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Load the model hyperparameters and initialize the model\n",
    "\n",
    "model      = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer  = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, seq len]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    #reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be using a `ReduceLROnPlateau` learning scheduler which decreases the learning rate by a factor, if the loss don't improve by a certain epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Perplexity: 296.300\n",
      "\tValid Perplexity: 199.111\n",
      "\tTrain loss : 5.691\tValid loss: 5.294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02\n",
      "\tTrain Perplexity: 197.982\n",
      "\tValid Perplexity: 146.724\n",
      "\tTrain loss : 5.288\tValid loss: 4.989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03\n",
      "\tTrain Perplexity: 157.263\n",
      "\tValid Perplexity: 125.858\n",
      "\tTrain loss : 5.058\tValid loss: 4.835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04\n",
      "\tTrain Perplexity: 135.572\n",
      "\tValid Perplexity: 113.747\n",
      "\tTrain loss : 4.910\tValid loss: 4.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05\n",
      "\tTrain Perplexity: 121.389\n",
      "\tValid Perplexity: 106.186\n",
      "\tTrain loss : 4.799\tValid loss: 4.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06\n",
      "\tTrain Perplexity: 111.017\n",
      "\tValid Perplexity: 101.458\n",
      "\tTrain loss : 4.710\tValid loss: 4.620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07\n",
      "\tTrain Perplexity: 103.012\n",
      "\tValid Perplexity: 97.178\n",
      "\tTrain loss : 4.635\tValid loss: 4.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08\n",
      "\tTrain Perplexity: 96.304\n",
      "\tValid Perplexity: 93.496\n",
      "\tTrain loss : 4.568\tValid loss: 4.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09\n",
      "\tTrain Perplexity: 90.529\n",
      "\tValid Perplexity: 91.320\n",
      "\tTrain loss : 4.506\tValid loss: 4.514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "\tTrain Perplexity: 85.590\n",
      "\tValid Perplexity: 89.120\n",
      "\tTrain loss : 4.450\tValid loss: 4.490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\n",
      "\tTrain Perplexity: 81.092\n",
      "\tValid Perplexity: 87.706\n",
      "\tTrain loss : 4.396\tValid loss: 4.474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\n",
      "\tTrain Perplexity: 77.302\n",
      "\tValid Perplexity: 86.565\n",
      "\tTrain loss : 4.348\tValid loss: 4.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13\n",
      "\tTrain Perplexity: 73.619\n",
      "\tValid Perplexity: 85.491\n",
      "\tTrain loss : 4.299\tValid loss: 4.448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14\n",
      "\tTrain Perplexity: 70.490\n",
      "\tValid Perplexity: 85.034\n",
      "\tTrain loss : 4.255\tValid loss: 4.443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15\n",
      "\tTrain Perplexity: 67.627\n",
      "\tValid Perplexity: 84.737\n",
      "\tTrain loss : 4.214\tValid loss: 4.440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16\n",
      "\tTrain Perplexity: 64.959\n",
      "\tValid Perplexity: 84.536\n",
      "\tTrain loss : 4.174\tValid loss: 4.437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17\n",
      "\tTrain Perplexity: 62.442\n",
      "\tValid Perplexity: 84.308\n",
      "\tTrain loss : 4.134\tValid loss: 4.434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18\n",
      "\tTrain Perplexity: 60.160\n",
      "\tValid Perplexity: 83.792\n",
      "\tTrain loss : 4.097\tValid loss: 4.428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19\n",
      "\tTrain Perplexity: 58.042\n",
      "\tValid Perplexity: 83.443\n",
      "\tTrain loss : 4.061\tValid loss: 4.424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20\n",
      "\tTrain Perplexity: 56.063\n",
      "\tValid Perplexity: 83.105\n",
      "\tTrain loss : 4.026\tValid loss: 4.420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21\n",
      "\tTrain Perplexity: 54.306\n",
      "\tValid Perplexity: 83.693\n",
      "\tTrain loss : 3.995\tValid loss: 4.427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22\n",
      "\tTrain Perplexity: 51.466\n",
      "\tValid Perplexity: 82.603\n",
      "\tTrain loss : 3.941\tValid loss: 4.414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23\n",
      "\tTrain Perplexity: 50.138\n",
      "\tValid Perplexity: 82.755\n",
      "\tTrain loss : 3.915\tValid loss: 4.416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24\n",
      "\tTrain Perplexity: 48.621\n",
      "\tValid Perplexity: 82.723\n",
      "\tTrain loss : 3.884\tValid loss: 4.416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25\n",
      "\tTrain Perplexity: 47.915\n",
      "\tValid Perplexity: 82.437\n",
      "\tTrain loss : 3.869\tValid loss: 4.412\n",
      "Training of 25 epochs were completed in 8m 35s.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 25\n",
    "seq_len  = 50 #<----decoding length\n",
    "clip    = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-lstm_lm_ian_fleming.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')\n",
    "    print(f'\\tTrain loss : {train_loss:.3f}\\tValid loss: {valid_loss:.3f}')\n",
    "    \n",
    "# Calculate the elapsed time\n",
    "end = time.time()\n",
    "epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "print(f\"Training of {n_epochs} epochs were completed in {epoch_mins}m {epoch_secs}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 82.327\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-val-lstm_lm_ian_fleming.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world inference\n",
    "\n",
    "Here we take the prompt, tokenize, encode and feed it into the model to get the predictions.  We then apply softmax while specifying that we want the output due to the last word in the sequence which represents the prediction for the next word.  We divide the logits by a temperature value to alter the modelâ€™s confidence by adjusting the softmax probability distribution.\n",
    "\n",
    "Once we have the Softmax distribution, we randomly sample it to make our prediction on the next word. If we get <unk> then we give that another try.  Once we get <eos> we stop predicting.\n",
    "    \n",
    "We decode the prediction back to strings last lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation\n",
    "\n",
    "Since james bond books contain mainly of action and fightings, our model will prompt best if we use words related to action scenes...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "their hands touched lightly and then he brushed it back to hers . he got out , and slit it into the sunshine and began filling her minutely in the air .\n",
      "\n",
      "0.85\n",
      "their hands touched lightly and then he brushed it back to hers .\n",
      "\n",
      "0.7\n",
      "their hands touched lightly and then he brushed the glass lever out of his pocket and watched it and sat down .\n",
      "\n",
      "0.6\n",
      "their hands touched lightly and then he brushed the glass lever out of his pocket and watched the bright powder of the engine . he threw up the safety-catch and cleaned the other back to his\n",
      "\n",
      "0.5\n",
      "their hands touched lightly and then he was wearing his hand , but he got to his feet and sat down .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# experimentation with prompting\n",
    "prompt = 'Their hands touched lightly and then'\n",
    "max_seq_len = 30\n",
    "seed = 69\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [1.0, 0.85, 0.7, 0.6, 0.5 ]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "\n",
    "It was found out that, after experimentation with different prompts, temperature 1.0 returns overfitted text.\n",
    "\n",
    "Temperature returns between .5 and .7 seems to return more natural responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
