{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Direct Preference Optimization: (DPO)]\n",
    "\n",
    "st125214 - Maung Maung Kyi Tha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the final dataset object should contain these 3 entries if you use the default DPODataCollatorWithPadding data collator. \n",
    "\n",
    "The entries should be named:\n",
    "- prompt\n",
    "- chosen\n",
    "- rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Available GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# setting device to GPU cuda if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device}\")\n",
    "print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Seet my seed\n",
    "SEED = 75\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Making sure we get the same results on each run\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Disable user warnings for neater output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear GPU cache at first run\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mgmgk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing HuggingFace libraries requried for the DPO model Training\n",
    "# Huggingface Datasets\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# Huggingface Transformers\n",
    "from transformers import ( AutoModelForCausalLM, AutoTokenizer, \n",
    "    HfArgumentParser, TrainingArguments )\n",
    "\n",
    "# Huggingface Trainer\n",
    "from typing import Dict, Optional\n",
    "from trl import DPOTrainer, DPOConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 : Finding a suitable dataset and preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset jondurbin/truthy-dpo-v0.1 is designed to enhance the truthfulness of large language models (LLMs) without compromising their ability to role-play as humans. It primarily targets areas such as corporeal, spatial, temporal awareness, and common misconceptions.​\n",
    "\n",
    "Key Features:\n",
    "    Format: Parquet​\n",
    "    Size: Between 1,000 and 10,000 samples​\n",
    "    License: CC BY 4.0​\n",
    "\n",
    "This dataset has been employed in fine-tuning models to improve their truthfulness. For instance, a user reported positive results after training a model using only the first 200 cases due to hardware constraints.\n",
    "\n",
    "For my case, I choose sentences from dataset which have character counts only between (30 ~ 300) to overcome resource limitation. I also used first 7 from the dataset to scale the process workable on my environment.\n",
    "\n",
    "https://huggingface.co/datasets/jondurbin/truthy-dpo-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for cleaning and preprocessing the loaded dataset sample\n",
    "def preprocess(sample: dict) -> dict:\n",
    "    \"\"\"Strips leading and trailing spaces from the text.\"\"\"\n",
    "    return {\n",
    "        \"prompt\": sample[\"prompt\"].strip(),\n",
    "        \"chosen\": sample[\"chosen\"].strip(),\n",
    "        \"rejected\": sample[\"rejected\"].strip(),\n",
    "    }\n",
    "\n",
    "# Function to filter samples based on character length\n",
    "def filter_samples(sample: dict, min_length: int = 30, max_length: int = 300) -> bool:\n",
    "    \"\"\"Filters samples where 'prompt', 'chosen', and 'rejected' are between min_length and max_length.\"\"\"\n",
    "    return (\n",
    "        min_length <= len(sample[\"prompt\"]) <= max_length and\n",
    "        min_length <= len(sample[\"chosen\"]) <= max_length and\n",
    "        min_length <= len(sample[\"rejected\"]) <= max_length\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load, filter, and split the dataset\n",
    "def get_hh(sanity_check: bool = False, cache_dir: str = None, test_size: float = 0.3) -> dict:\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\"jondurbin/truthy-dpo-v0.1\", split=\"train\", cache_dir=cache_dir)  # Load as a single set\n",
    "\n",
    "    # Debug: check dataset structure\n",
    "    print(dataset)\n",
    "\n",
    "    # Apply filtering to retain only short samples\n",
    "    dataset = dataset.filter(lambda sample: filter_samples(sample, max_length=300))\n",
    "\n",
    "    # Shuffle dataset before splitting\n",
    "    dataset = dataset.shuffle(seed=75)\n",
    "\n",
    "    # since this dataset has no predefined split, we will split it manually\n",
    "    # Split the dataset manually (70% train, 30% test)\n",
    "    train_size = int((1 - test_size) * len(dataset))\n",
    "    train_dataset = dataset.select(range(train_size))\n",
    "    test_dataset = dataset.select(range(train_size, len(dataset)))\n",
    "\n",
    "    # Limit dataset size for sanity check\n",
    "    d_size = 10  # Keep only 5 samples for testing if sanity_check is enabled\n",
    "    if sanity_check:\n",
    "        train_dataset = train_dataset.select(range(min(len(train_dataset), d_size)))\n",
    "        test_dataset = test_dataset.select(range(min(len(test_dataset), d_size)))\n",
    "\n",
    "    # Apply preprocessing\n",
    "    train_dataset = train_dataset.map(preprocess)\n",
    "    test_dataset = test_dataset.map(preprocess)\n",
    "\n",
    "    return {\"train\": train_dataset, \"test\": test_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'source', 'system', 'prompt', 'chosen', 'rejected'],\n",
      "    num_rows: 1016\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1eb6bfbea724409a792ed709b460f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1016 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513819d4448649388cd48b0807b3b360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962659bae96d4211b2eef8e3fd68eaf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Getting the training and evaluation datasets with sanity check\n",
    "sanity_check = True\n",
    "datasets = get_hh(sanity_check=sanity_check)\n",
    "train_dataset = datasets[\"train\"]\n",
    "eval_dataset = datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'source', 'system', 'prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my train dataset\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'source', 'system', 'prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my eval dataset\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Sample from Train Set:\n",
      "Prompt: What is the nearest historical site to your location?\n",
      "Chosen Response: Well, I'm currently residing in London, and there are numerous historical sites around. But the nearest one to me is the iconic Tower of London, a historic castle located on the north bank of the River Thames. It's a fascinating place with a rich history dating back to the Norman Conquest.\n",
      "Rejected Response: I am an AI language model and do not have access to my own location. However, if you provide me with your location, I can help you find the nearest historical site.\n"
     ]
    }
   ],
   "source": [
    "# Print some randomized samples\n",
    "random_index = random.randint(0, len(train_dataset) - 1)\n",
    "print(\"Random Sample from Train Set:\")\n",
    "print(\"Prompt:\", train_dataset[\"prompt\"][random_index])\n",
    "print(\"Chosen Response:\", train_dataset[\"chosen\"][random_index])\n",
    "print(\"Rejected Response:\", train_dataset[\"rejected\"][random_index])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 : Training a Model with DPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Qwen2-0.5B-Instruct model is an instruction-tuned language model developed by Alibaba Cloud. It is part of the Qwen2 series, which includes models of various sizes designed for tasks such as language understanding, generation, and multilingual applications. The Qwen2-0.5B-Instruct model specifically contains approximately 0.5 billion parameters and is fine-tuned to follow instructions effectively. It incorporates architectural features like the Transformer structure with SwiGLU activation, attention QKV bias, and group query attention. Additionally, it utilizes an improved tokenizer that adapts to multiple natural languages and code. The model has been pretrained on a large dataset and further refined through supervised fine-tuning and direct preference optimization. For optimal performance, it is recommended to use this model with Hugging Face's transformers library version 4.37.0 or later.\n",
    "\n",
    "https://huggingface.co/Qwen/Qwen2-0.5B-Instruct?utm_source=chatgpt.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# pre-trained 500M parameter instruction-tuned model\n",
    "model_name_or_path = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "\n",
    "# Creates a reference model for Direct Preference Optimization (DPO) training\n",
    "# This allows comparing the fine-tuned model (model) with the original model (ref_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Loads tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sends both model and ref_moe to the device for training\n",
    "model.to(device)\n",
    "ref_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters for the DPO model training\n",
    "\n",
    "learning_rates = [1e-5]     # set learning rate\n",
    "batch_sizes = [5]           # sets batch size\n",
    "num_epochs = [5]            # sets number of epochs\n",
    "betas = [0.1]               # sets beta value\n",
    "\n",
    "# beta is the temperature parameter that controls how strongly the preference signal is weighted in DPO training\n",
    "# beta = 0.0 corresponds to maximum likelihood training\n",
    "# beta = 1.0 corresponds to maximum preference training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all possible hyperparameter combinations through iteration\n",
    "\n",
    "hyperparameter_combinations = list(itertools.product(learning_rates, batch_sizes, num_epochs, betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for storing results of different hyperparameter configurations\n",
    "\n",
    "results = []                # Store results of different hyperparameter configurations\n",
    "best_loss = float(\"inf\")    # Initialize best loss as infinity (worst case)\n",
    "best_model_path = None      # Placeholder for the best model's saved path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training started : Learning Rate = 1e-05, Batch Size = 5, Epochs = 5, Beta = 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgmgk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b8be84765c4aa98a86fe1564e8de39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6536781eeae430fa34828ded681d3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe544d3ce808453a96dad2a7a6cc324e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b0c3f12d1f441abf2fa5894266ed6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in eval dataset:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949c64e6ee554c85a4f1a0c3766cbc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f544c48a35f4fbfa54f0f5bdb328214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 03:34, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.512166</td>\n",
       "      <td>0.384770</td>\n",
       "      <td>-0.347056</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.731826</td>\n",
       "      <td>-109.272079</td>\n",
       "      <td>-70.688965</td>\n",
       "      <td>-3.098582</td>\n",
       "      <td>-3.354053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.549814</td>\n",
       "      <td>0.178253</td>\n",
       "      <td>-0.942504</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.120757</td>\n",
       "      <td>-111.337250</td>\n",
       "      <td>-76.643448</td>\n",
       "      <td>-3.245224</td>\n",
       "      <td>-3.499057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.609029</td>\n",
       "      <td>-0.071853</td>\n",
       "      <td>-1.342673</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.270820</td>\n",
       "      <td>-113.838310</td>\n",
       "      <td>-80.645142</td>\n",
       "      <td>-3.348140</td>\n",
       "      <td>-3.613802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.639778</td>\n",
       "      <td>-0.209706</td>\n",
       "      <td>-1.563873</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.354168</td>\n",
       "      <td>-115.216835</td>\n",
       "      <td>-82.857140</td>\n",
       "      <td>-3.398449</td>\n",
       "      <td>-3.670166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.125800</td>\n",
       "      <td>0.649991</td>\n",
       "      <td>-0.259588</td>\n",
       "      <td>-1.644891</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.385303</td>\n",
       "      <td>-115.715668</td>\n",
       "      <td>-83.667313</td>\n",
       "      <td>-3.416048</td>\n",
       "      <td>-3.689985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model found! Saving model at: ./dpo_lr1e-05_bs5_ep5_beta0.1\n"
     ]
    }
   ],
   "source": [
    "# Iterate through all hyperparameter combinations\n",
    "for lr, batch_size, epochs, beta in hyperparameter_combinations:\n",
    "    print(f\"\\nTraining started : Learning Rate = {lr}, Batch Size = {batch_size}, Epochs = {epochs}, Beta = {beta}\")\n",
    "\n",
    "    # Creates a unique folder to save each trained model's outputs\n",
    "    output_dir = f\"./dpo_lr{lr}_bs{batch_size}_ep{epochs}_beta{beta}\"\n",
    "\n",
    "    # Configure DPO training parameters\n",
    "    dpo_config = DPOConfig(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",            # Evaluate model after each epoch\n",
    "        save_strategy=\"epoch\",                  # Save model after each epoch\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,                       # Log training progress every 10 steps\n",
    "        save_total_limit=2,                     # Save only the the last 2 saved checkpoints\n",
    "        learning_rate=lr,\n",
    "        report_to=\"none\",\n",
    "        beta=beta,\n",
    "        remove_unused_columns=False,            #  Prevents dropping dataset columns  \n",
    "    )\n",
    "\n",
    "    # Initialize DPOTrainer with the model, reference model, and DPO configuration\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=ref_model,\n",
    "        args=dpo_config,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Train the model with DPO\n",
    "    dpo_trainer.train()\n",
    "\n",
    "    # Evaluate the model on the eval set\n",
    "    eval_results = dpo_trainer.evaluate()\n",
    "    loss = eval_results.get(\"eval_loss\", None)\n",
    "    results.append({\n",
    "        \"learning_rate\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"beta\": beta,\n",
    "        \"loss\": loss\n",
    "    })\n",
    "\n",
    "    # Track the best model based on the lowest loss\n",
    "    if loss is not None and loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_model_path = output_dir\n",
    "        print(f\"New best model found! Saving model at: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./dpo_finetuned_model\\\\tokenizer_config.json',\n",
       " './dpo_finetuned_model\\\\special_tokens_map.json',\n",
       " './dpo_finetuned_model\\\\vocab.json',\n",
       " './dpo_finetuned_model\\\\merges.txt',\n",
       " './dpo_finetuned_model\\\\added_tokens.json',\n",
       " './dpo_finetuned_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer.\n",
    "model.save_pretrained(\"./dpo_finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./dpo_finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./dpo_finetuned_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./dpo_finetuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating response\n",
    "def generate_response(prompt: str, max_length: int = 250) -> str:\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "    \n",
    "    # Generate the response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode the generated output\n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remove the prompt from the generated text\n",
    "    response = full_text[len(prompt):].strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference Testing:\n",
      "\n",
      "Prompt: How can I influence in the public?\n",
      "\n",
      "Response: - It's a question that many people ask. - It depends on what one is trying to achieve. - The answer varies depending on the context. - I don't have a question, but I can provide an answer.\n",
      "Influence can be influenced in several ways, and it depends on the individual's goals and priorities. Here are some ways that can be influenced:\n",
      "\n",
      "1. Personal growth: If someone wants to influence in the public, they might start by personal growth. They could work on their own self-awareness, develop a sense of responsibility, or make positive changes in their community.\n",
      "\n",
      "2. Civic engagement: If someone wants to influence in the public, they could also consider civic engagement. This means being involved in local issues, volunteering, and advocating for social justice causes.\n",
      "\n",
      "3. Leadership: If someone wants to influence in the public, they might consider becoming a leader. This involves setting a positive example, inspiring others, and leading by example.\n",
      "\n",
      "4. Social media: Social media platforms can be a powerful tool for influencing in the public. By using these platforms effectively, individuals can reach a wider audience and promote positive change.\n",
      "\n",
      "5. Education: Education plays a crucial role in influencing in the public\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How can I influence in the public?\"\n",
    "print(\"\\nInference Testing:\")\n",
    "print(\"\\nPrompt:\", prompt)\n",
    "print(\"\\nResponse:\", generate_response(prompt, max_length=250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 : Pushing the Model to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries required for uploading the trained model to Huggingface Hub\n",
    "from huggingface_hub import create_repo, login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9d9f23f1944451a7fe489328b84191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here is the code to login to Huggingface Hub using the API token\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a3919d285f49dbb946dc17dc769be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5354713675414470b8b693896524d849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233d50a964704d1ea63465f84f30aba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully uploaded to: https://huggingface.co/mgmgkyit/dpo_finetuned_model\n"
     ]
    }
   ],
   "source": [
    "repo_id = 'mgmgkyit/dpo_finetuned_model'\n",
    "create_repo(repo_id, repo_type='model', private=False, exist_ok=True)\n",
    "\n",
    "# Push the dataset to Hugging Face\n",
    "model.push_to_hub(repo_id, safe_serialization=False)\n",
    "tokenizer.push_to_hub(repo_id)\n",
    "\n",
    "print(f\"Model successfully uploaded to: https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
